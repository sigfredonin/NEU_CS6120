====December 10, 2018 06:55:03 PM====
------------------------------- Hyper-parameters -------------------------------
USE_FULL_TRAIN: False, TRAIN_SIZE: 20000
Full training set size - sarcastic: 20000, non-sarcastic: 100000
Senti-scores - # most common n-grams: 25000, remove common n-grams: False
Senti-scores - min senses: 3, max senses: 12
Senti-scores - remove stopwords: False remove punctuation: False
Senti-scores - punctuation: ... : .. . , ;
--------------------------------------------------------------------------------
num sarcastic tweets: 25273
num non sarcastic tweets: 117825
Scoring sentiment in 20000 tweets ...
..................................................
..................................................
..................................................
..................................................
[(0, 2892), (2, 1418), (1, 1205), (3, 1015), (-2, 959), (4, 903), (-1, 888), (5, 829), (-3, 819), (-4, 621), (6, 557), (7, 490), (-5, 490), (8, 424), (11, 386), (9, 378), (10, 339), (-6, 337), (13, 309), (12, 274)]
Tweets with scored words: 18525; total words scored: 63595
Tweets with exceptions: 19729; total exceptions: 1138583
Total word/tags with scores: 5060
Scoring sentiment in 23098 tweets ...
..................................................
..................................................
..................................................
..................................................
..............................[(0, 3412), (2, 1735), (1, 1443), (-2, 1284), (3, 1184), (-1, 1128), (4, 1064), (-3, 990), (5, 905), (-4, 753), (6, 680), (-5, 590), (7, 558), (8, 452), (-6, 445), (11, 395), (9, 368), (10, 352), (-7, 335), (-8, 307)]
Tweets with scored words: 21341; total words scored: 73110
Tweets with exceptions: 22800; total exceptions: 1291988
Total word/tags with scores: 6833
Training features:
[[ 8  7  8  3  0  4 -8]
 [20 19 20  7  0  7 -3]]
Training labels:
[1 1 1 1 1 1 1 1 1 1]
Testing features:
[[13  3 13  3  0  0  1]
 [ 6  2  6  1  0  5  0]]
Testing labels:
[1 1 1 1 1 1 1 1 1 1]
====December 10, 2018 06:58:42 PM====
++++++++++++++++++++++++++++ Multilayer Perceptron +++++++++++++++++++++++++++++
Train and predict ...
====December 10, 2018 07:01:52 PM====
--- Model ---
Input    : 1 x 7
Layer h1 : 7 x 20 relu
Layer h2 : 20 x 10 relu
Output   : 10 x 1 SIGMOID
----------
====December 10, 2018 07:01:52 PM====
Epoch 1/10

   32/20000 [..............................] - ETA: 3:04 - loss: 2.6693 - mean_squared_error: 0.3669 - acc: 0.6250
 2080/20000 [==>...........................] - ETA: 3s - loss: 1.1640 - mean_squared_error: 0.3283 - acc: 0.5202  
 3968/20000 [====>.........................] - ETA: 1s - loss: 0.8696 - mean_squared_error: 0.2579 - acc: 0.6187
 5984/20000 [=======>......................] - ETA: 1s - loss: 0.7283 - mean_squared_error: 0.2189 - acc: 0.6811
 7968/20000 [==========>...................] - ETA: 0s - loss: 0.6445 - mean_squared_error: 0.1949 - acc: 0.7195
10528/20000 [==============>...............] - ETA: 0s - loss: 0.5745 - mean_squared_error: 0.1747 - acc: 0.7507
12960/20000 [==================>...........] - ETA: 0s - loss: 0.5313 - mean_squared_error: 0.1622 - acc: 0.7689
15456/20000 [======================>.......] - ETA: 0s - loss: 0.4962 - mean_squared_error: 0.1518 - acc: 0.7837
17536/20000 [=========================>....] - ETA: 0s - loss: 0.4729 - mean_squared_error: 0.1450 - acc: 0.7932
19936/20000 [============================>.] - ETA: 0s - loss: 0.4530 - mean_squared_error: 0.1393 - acc: 0.8017
20000/20000 [==============================] - 1s 40us/step - loss: 0.4527 - mean_squared_error: 0.1393 - acc: 0.8018
Epoch 2/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2506 - mean_squared_error: 0.0759 - acc: 0.9375
 2464/20000 [==>...........................] - ETA: 0s - loss: 0.3043 - mean_squared_error: 0.0972 - acc: 0.8608
 5024/20000 [======>.......................] - ETA: 0s - loss: 0.2970 - mean_squared_error: 0.0943 - acc: 0.8658
 7520/20000 [==========>...................] - ETA: 0s - loss: 0.3003 - mean_squared_error: 0.0956 - acc: 0.8633
 9536/20000 [=============>................] - ETA: 0s - loss: 0.2991 - mean_squared_error: 0.0955 - acc: 0.8624
11808/20000 [================>.............] - ETA: 0s - loss: 0.2951 - mean_squared_error: 0.0941 - acc: 0.8647
14400/20000 [====================>.........] - ETA: 0s - loss: 0.2924 - mean_squared_error: 0.0933 - acc: 0.8657
16768/20000 [========================>.....] - ETA: 0s - loss: 0.2888 - mean_squared_error: 0.0920 - acc: 0.8676
19296/20000 [===========================>..] - ETA: 0s - loss: 0.2898 - mean_squared_error: 0.0922 - acc: 0.8673
20000/20000 [==============================] - 0s 25us/step - loss: 0.2902 - mean_squared_error: 0.0924 - acc: 0.8675
Epoch 3/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3990 - mean_squared_error: 0.1284 - acc: 0.7812
 2464/20000 [==>...........................] - ETA: 0s - loss: 0.2841 - mean_squared_error: 0.0902 - acc: 0.8681
 4544/20000 [=====>........................] - ETA: 0s - loss: 0.2789 - mean_squared_error: 0.0892 - acc: 0.8699
 6560/20000 [========>.....................] - ETA: 0s - loss: 0.2858 - mean_squared_error: 0.0910 - acc: 0.8684
 9088/20000 [============>.................] - ETA: 0s - loss: 0.2800 - mean_squared_error: 0.0887 - acc: 0.8725
11072/20000 [===============>..............] - ETA: 0s - loss: 0.2810 - mean_squared_error: 0.0891 - acc: 0.8719
13728/20000 [===================>..........] - ETA: 0s - loss: 0.2804 - mean_squared_error: 0.0891 - acc: 0.8711
16320/20000 [=======================>......] - ETA: 0s - loss: 0.2810 - mean_squared_error: 0.0894 - acc: 0.8712
18880/20000 [===========================>..] - ETA: 0s - loss: 0.2837 - mean_squared_error: 0.0904 - acc: 0.8701
20000/20000 [==============================] - 0s 24us/step - loss: 0.2840 - mean_squared_error: 0.0905 - acc: 0.8700
Epoch 4/10

   32/20000 [..............................] - ETA: 0s - loss: 0.1732 - mean_squared_error: 0.0486 - acc: 0.9375
 1888/20000 [=>............................] - ETA: 0s - loss: 0.3042 - mean_squared_error: 0.0970 - acc: 0.8607
 3936/20000 [====>.........................] - ETA: 0s - loss: 0.2930 - mean_squared_error: 0.0934 - acc: 0.8651
 6080/20000 [========>.....................] - ETA: 0s - loss: 0.2896 - mean_squared_error: 0.0926 - acc: 0.8663
 8608/20000 [===========>..................] - ETA: 0s - loss: 0.2859 - mean_squared_error: 0.0915 - acc: 0.8679
11168/20000 [===============>..............] - ETA: 0s - loss: 0.2839 - mean_squared_error: 0.0909 - acc: 0.8689
13728/20000 [===================>..........] - ETA: 0s - loss: 0.2832 - mean_squared_error: 0.0906 - acc: 0.8687
16256/20000 [=======================>......] - ETA: 0s - loss: 0.2842 - mean_squared_error: 0.0907 - acc: 0.8692
18752/20000 [===========================>..] - ETA: 0s - loss: 0.2819 - mean_squared_error: 0.0899 - acc: 0.8700
20000/20000 [==============================] - 1s 25us/step - loss: 0.2830 - mean_squared_error: 0.0902 - acc: 0.8695
Epoch 5/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3144 - mean_squared_error: 0.1088 - acc: 0.8438
 2080/20000 [==>...........................] - ETA: 0s - loss: 0.2865 - mean_squared_error: 0.0917 - acc: 0.8678
 4128/20000 [=====>........................] - ETA: 0s - loss: 0.2843 - mean_squared_error: 0.0903 - acc: 0.8704
 6272/20000 [========>.....................] - ETA: 0s - loss: 0.2758 - mean_squared_error: 0.0878 - acc: 0.8755
 8768/20000 [============>.................] - ETA: 0s - loss: 0.2782 - mean_squared_error: 0.0882 - acc: 0.8760
11328/20000 [===============>..............] - ETA: 0s - loss: 0.2804 - mean_squared_error: 0.0889 - acc: 0.8746
13856/20000 [===================>..........] - ETA: 0s - loss: 0.2816 - mean_squared_error: 0.0896 - acc: 0.8730
16448/20000 [=======================>......] - ETA: 0s - loss: 0.2826 - mean_squared_error: 0.0900 - acc: 0.8730
18976/20000 [===========================>..] - ETA: 0s - loss: 0.2818 - mean_squared_error: 0.0896 - acc: 0.8735
20000/20000 [==============================] - 0s 24us/step - loss: 0.2835 - mean_squared_error: 0.0902 - acc: 0.8722
Epoch 6/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2489 - mean_squared_error: 0.0851 - acc: 0.8750
 2080/20000 [==>...........................] - ETA: 0s - loss: 0.2850 - mean_squared_error: 0.0915 - acc: 0.8692
 4160/20000 [=====>........................] - ETA: 0s - loss: 0.2764 - mean_squared_error: 0.0882 - acc: 0.8733
 6368/20000 [========>.....................] - ETA: 0s - loss: 0.2756 - mean_squared_error: 0.0878 - acc: 0.8733
 8928/20000 [============>.................] - ETA: 0s - loss: 0.2772 - mean_squared_error: 0.0882 - acc: 0.8730
11008/20000 [===============>..............] - ETA: 0s - loss: 0.2762 - mean_squared_error: 0.0881 - acc: 0.8724
13024/20000 [==================>...........] - ETA: 0s - loss: 0.2776 - mean_squared_error: 0.0886 - acc: 0.8721
15104/20000 [=====================>........] - ETA: 0s - loss: 0.2805 - mean_squared_error: 0.0893 - acc: 0.8720
17088/20000 [========================>.....] - ETA: 0s - loss: 0.2816 - mean_squared_error: 0.0897 - acc: 0.8709
19200/20000 [===========================>..] - ETA: 0s - loss: 0.2808 - mean_squared_error: 0.0895 - acc: 0.8711
20000/20000 [==============================] - 1s 25us/step - loss: 0.2815 - mean_squared_error: 0.0897 - acc: 0.8708
Epoch 7/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2736 - mean_squared_error: 0.0915 - acc: 0.8750
 2304/20000 [==>...........................] - ETA: 0s - loss: 0.2767 - mean_squared_error: 0.0888 - acc: 0.8685
 4800/20000 [======>.......................] - ETA: 0s - loss: 0.2794 - mean_squared_error: 0.0892 - acc: 0.8681
 7136/20000 [=========>....................] - ETA: 0s - loss: 0.2768 - mean_squared_error: 0.0883 - acc: 0.8694
 9344/20000 [=============>................] - ETA: 0s - loss: 0.2789 - mean_squared_error: 0.0889 - acc: 0.8701
11840/20000 [================>.............] - ETA: 0s - loss: 0.2806 - mean_squared_error: 0.0896 - acc: 0.8693
14336/20000 [====================>.........] - ETA: 0s - loss: 0.2796 - mean_squared_error: 0.0892 - acc: 0.8705
16352/20000 [=======================>......] - ETA: 0s - loss: 0.2796 - mean_squared_error: 0.0893 - acc: 0.8709
19008/20000 [===========================>..] - ETA: 0s - loss: 0.2802 - mean_squared_error: 0.0894 - acc: 0.8708
20000/20000 [==============================] - 0s 25us/step - loss: 0.2798 - mean_squared_error: 0.0892 - acc: 0.8716
Epoch 8/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3360 - mean_squared_error: 0.1182 - acc: 0.8125
 1632/20000 [=>............................] - ETA: 0s - loss: 0.2923 - mean_squared_error: 0.0922 - acc: 0.8725
 3648/20000 [====>.........................] - ETA: 0s - loss: 0.2802 - mean_squared_error: 0.0886 - acc: 0.8725
 5920/20000 [=======>......................] - ETA: 0s - loss: 0.2837 - mean_squared_error: 0.0895 - acc: 0.8720
 8448/20000 [===========>..................] - ETA: 0s - loss: 0.2869 - mean_squared_error: 0.0912 - acc: 0.8673
10944/20000 [===============>..............] - ETA: 0s - loss: 0.2877 - mean_squared_error: 0.0916 - acc: 0.8678
13376/20000 [===================>..........] - ETA: 0s - loss: 0.2859 - mean_squared_error: 0.0911 - acc: 0.8680
15936/20000 [======================>.......] - ETA: 0s - loss: 0.2842 - mean_squared_error: 0.0906 - acc: 0.8680
17984/20000 [=========================>....] - ETA: 0s - loss: 0.2821 - mean_squared_error: 0.0901 - acc: 0.8687
20000/20000 [==============================] - 1s 25us/step - loss: 0.2808 - mean_squared_error: 0.0896 - acc: 0.8693
Epoch 9/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3081 - mean_squared_error: 0.0955 - acc: 0.8750
 2496/20000 [==>...........................] - ETA: 0s - loss: 0.2816 - mean_squared_error: 0.0906 - acc: 0.8686
 4576/20000 [=====>........................] - ETA: 0s - loss: 0.2762 - mean_squared_error: 0.0883 - acc: 0.8739
 7200/20000 [=========>....................] - ETA: 0s - loss: 0.2776 - mean_squared_error: 0.0887 - acc: 0.8733
 9728/20000 [=============>................] - ETA: 0s - loss: 0.2776 - mean_squared_error: 0.0884 - acc: 0.8730
12224/20000 [=================>............] - ETA: 0s - loss: 0.2762 - mean_squared_error: 0.0879 - acc: 0.8728
14304/20000 [====================>.........] - ETA: 0s - loss: 0.2785 - mean_squared_error: 0.0885 - acc: 0.8726
16896/20000 [========================>.....] - ETA: 0s - loss: 0.2785 - mean_squared_error: 0.0885 - acc: 0.8728
19456/20000 [============================>.] - ETA: 0s - loss: 0.2804 - mean_squared_error: 0.0891 - acc: 0.8720
20000/20000 [==============================] - 0s 24us/step - loss: 0.2804 - mean_squared_error: 0.0891 - acc: 0.8720
Epoch 10/10

   32/20000 [..............................] - ETA: 9s - loss: 0.1984 - mean_squared_error: 0.0569 - acc: 0.9688
 2624/20000 [==>...........................] - ETA: 0s - loss: 0.2819 - mean_squared_error: 0.0904 - acc: 0.8681
 5184/20000 [======>.......................] - ETA: 0s - loss: 0.2767 - mean_squared_error: 0.0883 - acc: 0.8744
 7712/20000 [==========>...................] - ETA: 0s - loss: 0.2813 - mean_squared_error: 0.0894 - acc: 0.8724
10272/20000 [==============>...............] - ETA: 0s - loss: 0.2818 - mean_squared_error: 0.0897 - acc: 0.8728
12352/20000 [=================>............] - ETA: 0s - loss: 0.2821 - mean_squared_error: 0.0901 - acc: 0.8714
15040/20000 [=====================>........] - ETA: 0s - loss: 0.2806 - mean_squared_error: 0.0896 - acc: 0.8718
17184/20000 [========================>.....] - ETA: 0s - loss: 0.2803 - mean_squared_error: 0.0894 - acc: 0.8720
19712/20000 [============================>.] - ETA: 0s - loss: 0.2813 - mean_squared_error: 0.0898 - acc: 0.8713
20000/20000 [==============================] - 0s 25us/step - loss: 0.2813 - mean_squared_error: 0.0898 - acc: 0.8715
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 1 1 0 0 0 0 1 1] [0 1 1 0 0 0 0 0 0 1]
... MSE: 0.305784 PEARSON: (0.2571538864113507, 0.0) F-SCORE: 0.451588
+++++++++++++++++++++++++++++++++ Max Entropy ++++++++++++++++++++++++++++++++++
Train and predict ...
training max_ent with penalty=l2, solver=sag, C=1.0, and class_weight=balanced
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [1 1 1 1 0 0 0 0 1 1] [0 1 1 0 0 0 0 0 0 1]
mean squared error: 0.34267036107022253
pearson coefficient: (0.24857091952666804, 2.87e-322)
f-score: 0.45327070525661395
... MSE: 0.342670 PEARSON: (0.24857091952666804, 2.87e-322) F-SCORE: 0.453271
++++++++++++++++++++++++++++ Support Vector Machine ++++++++++++++++++++++++++++
Train and predict ...
training svm with kernel='rbf', C=1.0, gamma='scale', class_weight='balanced'
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [1 1 1 1 0 0 1 0 1 1] [0 1 1 0 0 0 0 0 0 1]
mean squared error: 0.3527145207377262
pearson coefficient: (0.24964457376421376, 0.0)
f-score: 0.4553052082636892
... MSE: 0.352715 PEARSON: (0.24964457376421376, 0.0) F-SCORE: 0.455305
====December 10, 2018 07:02:09 PM====
