====December 10, 2018 02:04:20 PM====
------------------------------- Hyper-parameters -------------------------------
USE_FULL_TRAIN: False, TRAIN_SIZE: 20000
Full training set size - sarcastic: 20000, non-sarcastic: 100000
Senti-scores - # most common n-grams: 25000, remove common n-grams: True
Senti-scores - min senses: 3, max senses: 12
Senti-scores - remove stopwords: False remove punctuation: False
Senti-scores - punctuation: .. ; : , ... .
--------------------------------------------------------------------------------
num sarcastic tweets: 25273
num non sarcastic tweets: 117825
Scoring sentiment in 20000 tweets ...
..................................................
..................................................
..................................................
..................................................
[(0, 2892), (2, 1418), (1, 1205), (3, 1015), (-2, 959), (4, 903), (-1, 888), (5, 829), (-3, 819), (-4, 621), (6, 557), (7, 490), (-5, 490), (8, 424), (11, 386), (9, 378), (10, 339), (-6, 337), (13, 309), (12, 274)]
Tweets with scored words: 18525; total words scored: 63595
Tweets with exceptions: 19729; total exceptions: 1138583
Total word/tags with scores: 5060
Scoring sentiment in 20000 tweets ...
..................................................
..................................................
..................................................
..................................................
[(0, 2935), (2, 1514), (1, 1237), (-2, 1114), (3, 1017), (-1, 976), (4, 919), (-3, 846), (5, 770), (-4, 641), (6, 586), (-5, 519), (7, 498), (8, 392), (-6, 388), (11, 347), (9, 320), (10, 301), (-7, 296), (-8, 265)]
Tweets with scored words: 18503; total words scored: 63445
Tweets with exceptions: 19742; total exceptions: 1117778
Total word/tags with scores: 6656
Training features:
[[ 6  6  6  0  0  4 -8]
 [13 15 13  2  0  7 -3]]
Training labels:
[1 1 1 1 1 1 1 1 1 1]
Testing features:
[[7 1 7 2 0 0 1]
 [4 2 4 0 0 5 0]]
Testing labels:
[1 1 1 1 1 1 1 1 1 1]
====December 10, 2018 02:07:46 PM====
++++++++++++++++++++++++++++ Multilayer Perceptron +++++++++++++++++++++++++++++
Train and predict ...
====December 10, 2018 02:07:47 PM====
--- Model ---
Input    : 1 x 7
Layer h1 : 7 x 20 relu
Layer h2 : 20 x 10 relu
Output   : 10 x 1 SIGMOID
----------
====December 10, 2018 02:07:47 PM====
Epoch 1/10

   32/20000 [..............................] - ETA: 3:01 - loss: 0.5814 - mean_squared_error: 0.1970 - acc: 0.7500
 2240/20000 [==>...........................] - ETA: 2s - loss: 0.4753 - mean_squared_error: 0.1534 - acc: 0.7973  
 4640/20000 [=====>........................] - ETA: 1s - loss: 0.4167 - mean_squared_error: 0.1308 - acc: 0.8304
 7104/20000 [=========>....................] - ETA: 0s - loss: 0.3710 - mean_squared_error: 0.1153 - acc: 0.8508
 9600/20000 [=============>................] - ETA: 0s - loss: 0.3472 - mean_squared_error: 0.1077 - acc: 0.8581
12128/20000 [=================>............] - ETA: 0s - loss: 0.3363 - mean_squared_error: 0.1042 - acc: 0.8611
14656/20000 [====================>.........] - ETA: 0s - loss: 0.3299 - mean_squared_error: 0.1018 - acc: 0.8644
17184/20000 [========================>.....] - ETA: 0s - loss: 0.3198 - mean_squared_error: 0.0986 - acc: 0.8688
19680/20000 [============================>.] - ETA: 0s - loss: 0.3167 - mean_squared_error: 0.0977 - acc: 0.8689
20000/20000 [==============================] - 1s 40us/step - loss: 0.3162 - mean_squared_error: 0.0975 - acc: 0.8689
Epoch 2/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3174 - mean_squared_error: 0.0828 - acc: 0.9062
 2080/20000 [==>...........................] - ETA: 0s - loss: 0.2920 - mean_squared_error: 0.0901 - acc: 0.8745
 4096/20000 [=====>........................] - ETA: 0s - loss: 0.2828 - mean_squared_error: 0.0863 - acc: 0.8801
 6272/20000 [========>.....................] - ETA: 0s - loss: 0.2772 - mean_squared_error: 0.0843 - acc: 0.8830
 8832/20000 [============>.................] - ETA: 0s - loss: 0.2760 - mean_squared_error: 0.0841 - acc: 0.8842
11328/20000 [===============>..............] - ETA: 0s - loss: 0.2785 - mean_squared_error: 0.0848 - acc: 0.8837
13856/20000 [===================>..........] - ETA: 0s - loss: 0.2755 - mean_squared_error: 0.0838 - acc: 0.8851
16384/20000 [=======================>......] - ETA: 0s - loss: 0.2771 - mean_squared_error: 0.0842 - acc: 0.8843
18464/20000 [==========================>...] - ETA: 0s - loss: 0.2775 - mean_squared_error: 0.0845 - acc: 0.8836
20000/20000 [==============================] - 0s 25us/step - loss: 0.2765 - mean_squared_error: 0.0843 - acc: 0.8842
Epoch 3/10

   32/20000 [..............................] - ETA: 0s - loss: 0.1734 - mean_squared_error: 0.0538 - acc: 0.9375
 2336/20000 [==>...........................] - ETA: 0s - loss: 0.2638 - mean_squared_error: 0.0801 - acc: 0.8878
 4800/20000 [======>.......................] - ETA: 0s - loss: 0.2682 - mean_squared_error: 0.0817 - acc: 0.8873
 7328/20000 [=========>....................] - ETA: 0s - loss: 0.2789 - mean_squared_error: 0.0852 - acc: 0.8821
 9824/20000 [=============>................] - ETA: 0s - loss: 0.2761 - mean_squared_error: 0.0841 - acc: 0.8840
11840/20000 [================>.............] - ETA: 0s - loss: 0.2740 - mean_squared_error: 0.0833 - acc: 0.8845
14112/20000 [====================>.........] - ETA: 0s - loss: 0.2709 - mean_squared_error: 0.0823 - acc: 0.8858
16640/20000 [=======================>......] - ETA: 0s - loss: 0.2743 - mean_squared_error: 0.0833 - acc: 0.8849
19200/20000 [===========================>..] - ETA: 0s - loss: 0.2739 - mean_squared_error: 0.0835 - acc: 0.8846
20000/20000 [==============================] - 0s 25us/step - loss: 0.2739 - mean_squared_error: 0.0836 - acc: 0.8845
Epoch 4/10

   32/20000 [..............................] - ETA: 0s - loss: 0.4301 - mean_squared_error: 0.1207 - acc: 0.8750
 2336/20000 [==>...........................] - ETA: 0s - loss: 0.2861 - mean_squared_error: 0.0844 - acc: 0.8866
 4416/20000 [=====>........................] - ETA: 0s - loss: 0.2835 - mean_squared_error: 0.0854 - acc: 0.8843
 6688/20000 [=========>....................] - ETA: 0s - loss: 0.2722 - mean_squared_error: 0.0816 - acc: 0.8897
 8704/20000 [============>.................] - ETA: 0s - loss: 0.2718 - mean_squared_error: 0.0819 - acc: 0.8892
11104/20000 [===============>..............] - ETA: 0s - loss: 0.2683 - mean_squared_error: 0.0813 - acc: 0.8894
13664/20000 [===================>..........] - ETA: 0s - loss: 0.2687 - mean_squared_error: 0.0814 - acc: 0.8894
16192/20000 [=======================>......] - ETA: 0s - loss: 0.2690 - mean_squared_error: 0.0817 - acc: 0.8888
18784/20000 [===========================>..] - ETA: 0s - loss: 0.2708 - mean_squared_error: 0.0824 - acc: 0.8869
20000/20000 [==============================] - 0s 24us/step - loss: 0.2711 - mean_squared_error: 0.0826 - acc: 0.8863
Epoch 5/10

   32/20000 [..............................] - ETA: 9s - loss: 0.2647 - mean_squared_error: 0.0516 - acc: 0.9688
 2560/20000 [==>...........................] - ETA: 0s - loss: 0.2822 - mean_squared_error: 0.0848 - acc: 0.8824
 5056/20000 [======>.......................] - ETA: 0s - loss: 0.2705 - mean_squared_error: 0.0821 - acc: 0.8855
 7360/20000 [==========>...................] - ETA: 0s - loss: 0.2643 - mean_squared_error: 0.0804 - acc: 0.8883
 9856/20000 [=============>................] - ETA: 0s - loss: 0.2693 - mean_squared_error: 0.0819 - acc: 0.8872
12352/20000 [=================>............] - ETA: 0s - loss: 0.2727 - mean_squared_error: 0.0829 - acc: 0.8862
14848/20000 [=====================>........] - ETA: 0s - loss: 0.2736 - mean_squared_error: 0.0834 - acc: 0.8850
17408/20000 [=========================>....] - ETA: 0s - loss: 0.2717 - mean_squared_error: 0.0828 - acc: 0.8859
20000/20000 [==============================] - 1s 26us/step - loss: 0.2696 - mean_squared_error: 0.0822 - acc: 0.8868
Epoch 6/10

   32/20000 [..............................] - ETA: 0s - loss: 0.1996 - mean_squared_error: 0.0639 - acc: 0.8750
 2592/20000 [==>...........................] - ETA: 0s - loss: 0.2875 - mean_squared_error: 0.0872 - acc: 0.8819
 4640/20000 [=====>........................] - ETA: 0s - loss: 0.2750 - mean_squared_error: 0.0834 - acc: 0.8884
 6944/20000 [=========>....................] - ETA: 0s - loss: 0.2693 - mean_squared_error: 0.0818 - acc: 0.8900
 9504/20000 [=============>................] - ETA: 0s - loss: 0.2688 - mean_squared_error: 0.0818 - acc: 0.8891
12064/20000 [=================>............] - ETA: 0s - loss: 0.2691 - mean_squared_error: 0.0819 - acc: 0.8878
14592/20000 [====================>.........] - ETA: 0s - loss: 0.2703 - mean_squared_error: 0.0825 - acc: 0.8861
16640/20000 [=======================>......] - ETA: 0s - loss: 0.2703 - mean_squared_error: 0.0824 - acc: 0.8862
18848/20000 [===========================>..] - ETA: 0s - loss: 0.2704 - mean_squared_error: 0.0825 - acc: 0.8857
20000/20000 [==============================] - 0s 25us/step - loss: 0.2701 - mean_squared_error: 0.0825 - acc: 0.8860
Epoch 7/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2325 - mean_squared_error: 0.0762 - acc: 0.9062
 2400/20000 [==>...........................] - ETA: 0s - loss: 0.2631 - mean_squared_error: 0.0824 - acc: 0.8854
 4896/20000 [======>.......................] - ETA: 0s - loss: 0.2667 - mean_squared_error: 0.0826 - acc: 0.8854
 7424/20000 [==========>...................] - ETA: 0s - loss: 0.2706 - mean_squared_error: 0.0833 - acc: 0.8858
 9920/20000 [=============>................] - ETA: 0s - loss: 0.2677 - mean_squared_error: 0.0817 - acc: 0.8883
12448/20000 [=================>............] - ETA: 0s - loss: 0.2671 - mean_squared_error: 0.0819 - acc: 0.8871
14912/20000 [=====================>........] - ETA: 0s - loss: 0.2668 - mean_squared_error: 0.0815 - acc: 0.8881
17440/20000 [=========================>....] - ETA: 0s - loss: 0.2679 - mean_squared_error: 0.0818 - acc: 0.8884
19936/20000 [============================>.] - ETA: 0s - loss: 0.2682 - mean_squared_error: 0.0818 - acc: 0.8879
20000/20000 [==============================] - 0s 25us/step - loss: 0.2680 - mean_squared_error: 0.0818 - acc: 0.8880
Epoch 8/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2279 - mean_squared_error: 0.0755 - acc: 0.9062
 2400/20000 [==>...........................] - ETA: 0s - loss: 0.2680 - mean_squared_error: 0.0810 - acc: 0.8854
 4384/20000 [=====>........................] - ETA: 0s - loss: 0.2634 - mean_squared_error: 0.0807 - acc: 0.8875
 6400/20000 [========>.....................] - ETA: 0s - loss: 0.2695 - mean_squared_error: 0.0817 - acc: 0.8877
 8416/20000 [===========>..................] - ETA: 0s - loss: 0.2682 - mean_squared_error: 0.0812 - acc: 0.8880
10720/20000 [===============>..............] - ETA: 0s - loss: 0.2684 - mean_squared_error: 0.0815 - acc: 0.8872
13248/20000 [==================>...........] - ETA: 0s - loss: 0.2672 - mean_squared_error: 0.0812 - acc: 0.8875
15744/20000 [======================>.......] - ETA: 0s - loss: 0.2685 - mean_squared_error: 0.0817 - acc: 0.8861
18304/20000 [==========================>...] - ETA: 0s - loss: 0.2683 - mean_squared_error: 0.0816 - acc: 0.8867
20000/20000 [==============================] - 0s 25us/step - loss: 0.2685 - mean_squared_error: 0.0819 - acc: 0.8859
Epoch 9/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3358 - mean_squared_error: 0.0692 - acc: 0.9375
 2080/20000 [==>...........................] - ETA: 0s - loss: 0.2746 - mean_squared_error: 0.0835 - acc: 0.8846
 4288/20000 [=====>........................] - ETA: 0s - loss: 0.2715 - mean_squared_error: 0.0830 - acc: 0.8839
 6240/20000 [========>.....................] - ETA: 0s - loss: 0.2726 - mean_squared_error: 0.0833 - acc: 0.8846
 8576/20000 [===========>..................] - ETA: 0s - loss: 0.2684 - mean_squared_error: 0.0821 - acc: 0.8857
11104/20000 [===============>..............] - ETA: 0s - loss: 0.2676 - mean_squared_error: 0.0813 - acc: 0.8867
13600/20000 [===================>..........] - ETA: 0s - loss: 0.2671 - mean_squared_error: 0.0815 - acc: 0.8861
16160/20000 [=======================>......] - ETA: 0s - loss: 0.2657 - mean_squared_error: 0.0810 - acc: 0.8871
18688/20000 [===========================>..] - ETA: 0s - loss: 0.2659 - mean_squared_error: 0.0812 - acc: 0.8875
20000/20000 [==============================] - 1s 25us/step - loss: 0.2674 - mean_squared_error: 0.0816 - acc: 0.8866
Epoch 10/10

   32/20000 [..............................] - ETA: 1s - loss: 0.1438 - mean_squared_error: 0.0435 - acc: 0.9375
 2112/20000 [==>...........................] - ETA: 0s - loss: 0.2603 - mean_squared_error: 0.0796 - acc: 0.8887
 4576/20000 [=====>........................] - ETA: 0s - loss: 0.2592 - mean_squared_error: 0.0791 - acc: 0.8881
 6624/20000 [========>.....................] - ETA: 0s - loss: 0.2661 - mean_squared_error: 0.0810 - acc: 0.8872
 9184/20000 [============>.................] - ETA: 0s - loss: 0.2639 - mean_squared_error: 0.0807 - acc: 0.8882
11648/20000 [================>.............] - ETA: 0s - loss: 0.2615 - mean_squared_error: 0.0798 - acc: 0.8899
14144/20000 [====================>.........] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.0802 - acc: 0.8894
16672/20000 [========================>.....] - ETA: 0s - loss: 0.2676 - mean_squared_error: 0.0816 - acc: 0.8871
18880/20000 [===========================>..] - ETA: 0s - loss: 0.2675 - mean_squared_error: 0.0815 - acc: 0.8879
20000/20000 [==============================] - 0s 24us/step - loss: 0.2672 - mean_squared_error: 0.0815 - acc: 0.8877
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 1 0 1 1] [0 0 1 0 0 0 0 0 1 1]
... MSE: 0.310800 PEARSON: (0.35994119889751036, 0.0) F-SCORE: 0.553320
+++++++++++++++++++++++++++++++++ Max Entropy ++++++++++++++++++++++++++++++++++
Train and predict ...
training max_ent with penalty=l2, solver=sag, C=1.0, and class_weight=balanced
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 0 1 1 1] [0 0 1 0 0 0 0 0 0 0]
mean squared error: 0.2973
pearson coefficient: (0.3613101086281082, 0.0)
f-score: 0.5529323308270676
... MSE: 0.297300 PEARSON: (0.3613101086281082, 0.0) F-SCORE: 0.552932
++++++++++++++++++++++++++++ Support Vector Machine ++++++++++++++++++++++++++++
Train and predict ...
training svm with kernel='rbf', C=1.0, gamma='scale', class_weight='balanced'
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 1 0 1 1] [0 0 1 0 0 0 0 0 0 1]
mean squared error: 0.297
pearson coefficient: (0.3703585362541989, 0.0)
f-score: 0.5592163846838824
... MSE: 0.297000 PEARSON: (0.3703585362541989, 0.0) F-SCORE: 0.559216
====December 10, 2018 02:08:02 PM====
