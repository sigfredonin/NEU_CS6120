====December 10, 2018 05:42:19 PM====
------------------------------- Hyper-parameters -------------------------------
USE_FULL_TRAIN: False, TRAIN_SIZE: 20000
Full training set size - sarcastic: 20000, non-sarcastic: 100000
Senti-scores - # most common n-grams: 25000, remove common n-grams: True
Senti-scores - min senses: 3, max senses: 12
Senti-scores - remove stopwords: False remove punctuation: False
Senti-scores - punctuation: ; . .. : ... ,
--------------------------------------------------------------------------------
num sarcastic tweets: 25273
num non sarcastic tweets: 117825
Scoring sentiment in 20000 tweets ...
..................................................
..................................................
..................................................
..................................................
[(0, 2892), (2, 1418), (1, 1205), (3, 1015), (-2, 959), (4, 903), (-1, 888), (5, 829), (-3, 819), (-4, 621), (6, 557), (7, 490), (-5, 490), (8, 424), (11, 386), (9, 378), (10, 339), (-6, 337), (13, 309), (12, 274)]
Tweets with scored words: 18525; total words scored: 63595
Tweets with exceptions: 19729; total exceptions: 1138583
Total word/tags with scores: 5060
Scoring sentiment in 23098 tweets ...
..................................................
..................................................
..................................................
..................................................
..............................[(0, 3412), (2, 1735), (1, 1443), (-2, 1284), (3, 1184), (-1, 1128), (4, 1064), (-3, 990), (5, 905), (-4, 753), (6, 680), (-5, 590), (7, 558), (8, 452), (-6, 445), (11, 395), (9, 368), (10, 352), (-7, 335), (-8, 307)]
Tweets with scored words: 21341; total words scored: 73110
Tweets with exceptions: 22800; total exceptions: 1291988
Total word/tags with scores: 6833
Training features:
[[ 6  6  6  0  0  4 -8]
 [13 15 13  2  0  7 -3]]
Training labels:
[1 1 1 1 1 1 1 1 1 1]
Testing features:
[[7 1 7 2 0 0 1]
 [4 2 4 0 0 5 0]]
Testing labels:
[1 1 1 1 1 1 1 1 1 1]
====December 10, 2018 05:45:42 PM====
++++++++++++++++++++++++++++ Multilayer Perceptron +++++++++++++++++++++++++++++
Train and predict ...
====December 10, 2018 05:45:44 PM====
--- Model ---
Input    : 1 x 7
Layer h1 : 7 x 60 relu
Layer h2 : 60 x 10 relu
Output   : 10 x 1 SIGMOID
----------
====December 10, 2018 05:45:44 PM====
Epoch 1/10

   32/20000 [..............................] - ETA: 3:25 - loss: 1.0463 - mean_squared_error: 0.2476 - acc: 0.5938
 2272/20000 [==>...........................] - ETA: 3s - loss: 0.4698 - mean_squared_error: 0.1433 - acc: 0.8019  
 4704/20000 [======>.......................] - ETA: 1s - loss: 0.3914 - mean_squared_error: 0.1202 - acc: 0.8340
 7072/20000 [=========>....................] - ETA: 0s - loss: 0.3580 - mean_squared_error: 0.1098 - acc: 0.8487
 9472/20000 [=============>................] - ETA: 0s - loss: 0.3386 - mean_squared_error: 0.1036 - acc: 0.8582
11648/20000 [================>.............] - ETA: 0s - loss: 0.3231 - mean_squared_error: 0.0988 - acc: 0.8652
14144/20000 [====================>.........] - ETA: 0s - loss: 0.3150 - mean_squared_error: 0.0963 - acc: 0.8682
16640/20000 [=======================>......] - ETA: 0s - loss: 0.3116 - mean_squared_error: 0.0951 - acc: 0.8704
19136/20000 [===========================>..] - ETA: 0s - loss: 0.3067 - mean_squared_error: 0.0936 - acc: 0.8724
20000/20000 [==============================] - 1s 42us/step - loss: 0.3045 - mean_squared_error: 0.0930 - acc: 0.8731
Epoch 2/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2719 - mean_squared_error: 0.0840 - acc: 0.9062
 1824/20000 [=>............................] - ETA: 0s - loss: 0.2564 - mean_squared_error: 0.0787 - acc: 0.8909
 4288/20000 [=====>........................] - ETA: 0s - loss: 0.2644 - mean_squared_error: 0.0802 - acc: 0.8888
 6400/20000 [========>.....................] - ETA: 0s - loss: 0.2626 - mean_squared_error: 0.0791 - acc: 0.8903
 8800/20000 [============>.................] - ETA: 0s - loss: 0.2686 - mean_squared_error: 0.0813 - acc: 0.8866
10944/20000 [===============>..............] - ETA: 0s - loss: 0.2671 - mean_squared_error: 0.0810 - acc: 0.8875
13408/20000 [===================>..........] - ETA: 0s - loss: 0.2688 - mean_squared_error: 0.0817 - acc: 0.8873
15776/20000 [======================>.......] - ETA: 0s - loss: 0.2736 - mean_squared_error: 0.0829 - acc: 0.8862
18208/20000 [==========================>...] - ETA: 0s - loss: 0.2754 - mean_squared_error: 0.0838 - acc: 0.8851
20000/20000 [==============================] - 1s 25us/step - loss: 0.2753 - mean_squared_error: 0.0838 - acc: 0.8854
Epoch 3/10

   32/20000 [..............................] - ETA: 9s - loss: 0.1972 - mean_squared_error: 0.0558 - acc: 0.9062
 2016/20000 [==>...........................] - ETA: 0s - loss: 0.2712 - mean_squared_error: 0.0821 - acc: 0.8829
 4448/20000 [=====>........................] - ETA: 0s - loss: 0.2727 - mean_squared_error: 0.0821 - acc: 0.8853
 6880/20000 [=========>....................] - ETA: 0s - loss: 0.2731 - mean_squared_error: 0.0829 - acc: 0.8849
 9344/20000 [=============>................] - ETA: 0s - loss: 0.2700 - mean_squared_error: 0.0817 - acc: 0.8881
11776/20000 [================>.............] - ETA: 0s - loss: 0.2749 - mean_squared_error: 0.0839 - acc: 0.8846
14240/20000 [====================>.........] - ETA: 0s - loss: 0.2741 - mean_squared_error: 0.0834 - acc: 0.8857
16224/20000 [=======================>......] - ETA: 0s - loss: 0.2749 - mean_squared_error: 0.0838 - acc: 0.8852
18336/20000 [==========================>...] - ETA: 0s - loss: 0.2740 - mean_squared_error: 0.0838 - acc: 0.8849
20000/20000 [==============================] - 1s 26us/step - loss: 0.2717 - mean_squared_error: 0.0830 - acc: 0.8859
Epoch 4/10

   32/20000 [..............................] - ETA: 0s - loss: 0.1460 - mean_squared_error: 0.0451 - acc: 0.9375
 2368/20000 [==>...........................] - ETA: 0s - loss: 0.2709 - mean_squared_error: 0.0827 - acc: 0.8839
 4768/20000 [======>.......................] - ETA: 0s - loss: 0.2644 - mean_squared_error: 0.0810 - acc: 0.8863
 7232/20000 [=========>....................] - ETA: 0s - loss: 0.2610 - mean_squared_error: 0.0798 - acc: 0.8884
 9696/20000 [=============>................] - ETA: 0s - loss: 0.2641 - mean_squared_error: 0.0806 - acc: 0.8890
11648/20000 [================>.............] - ETA: 0s - loss: 0.2630 - mean_squared_error: 0.0801 - acc: 0.8904
13920/20000 [===================>..........] - ETA: 0s - loss: 0.2688 - mean_squared_error: 0.0817 - acc: 0.8877
15904/20000 [======================>.......] - ETA: 0s - loss: 0.2702 - mean_squared_error: 0.0822 - acc: 0.8878
17824/20000 [=========================>....] - ETA: 0s - loss: 0.2708 - mean_squared_error: 0.0824 - acc: 0.8871
20000/20000 [==============================] - 1s 25us/step - loss: 0.2697 - mean_squared_error: 0.0823 - acc: 0.8867
Epoch 5/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3263 - mean_squared_error: 0.1099 - acc: 0.7812
 1952/20000 [=>............................] - ETA: 0s - loss: 0.2450 - mean_squared_error: 0.0747 - acc: 0.8981
 3968/20000 [====>.........................] - ETA: 0s - loss: 0.2635 - mean_squared_error: 0.0794 - acc: 0.8924
 6016/20000 [========>.....................] - ETA: 0s - loss: 0.2604 - mean_squared_error: 0.0789 - acc: 0.8930
 8000/20000 [===========>..................] - ETA: 0s - loss: 0.2660 - mean_squared_error: 0.0809 - acc: 0.8884
10176/20000 [==============>...............] - ETA: 0s - loss: 0.2631 - mean_squared_error: 0.0800 - acc: 0.8900
12192/20000 [=================>............] - ETA: 0s - loss: 0.2659 - mean_squared_error: 0.0812 - acc: 0.8875
14720/20000 [=====================>........] - ETA: 0s - loss: 0.2683 - mean_squared_error: 0.0820 - acc: 0.8859
17280/20000 [========================>.....] - ETA: 0s - loss: 0.2680 - mean_squared_error: 0.0821 - acc: 0.8859
19840/20000 [============================>.] - ETA: 0s - loss: 0.2665 - mean_squared_error: 0.0814 - acc: 0.8874
20000/20000 [==============================] - 1s 25us/step - loss: 0.2668 - mean_squared_error: 0.0815 - acc: 0.8871
Epoch 6/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2880 - mean_squared_error: 0.0903 - acc: 0.9062
 2368/20000 [==>...........................] - ETA: 0s - loss: 0.2685 - mean_squared_error: 0.0832 - acc: 0.8839
 4352/20000 [=====>........................] - ETA: 0s - loss: 0.2679 - mean_squared_error: 0.0822 - acc: 0.8869
 6880/20000 [=========>....................] - ETA: 0s - loss: 0.2704 - mean_squared_error: 0.0830 - acc: 0.8842
 8896/20000 [============>.................] - ETA: 0s - loss: 0.2721 - mean_squared_error: 0.0834 - acc: 0.8838
11072/20000 [===============>..............] - ETA: 0s - loss: 0.2709 - mean_squared_error: 0.0829 - acc: 0.8848
13088/20000 [==================>...........] - ETA: 0s - loss: 0.2666 - mean_squared_error: 0.0814 - acc: 0.8864
15104/20000 [=====================>........] - ETA: 0s - loss: 0.2663 - mean_squared_error: 0.0815 - acc: 0.8863
17152/20000 [========================>.....] - ETA: 0s - loss: 0.2675 - mean_squared_error: 0.0819 - acc: 0.8857
19840/20000 [============================>.] - ETA: 0s - loss: 0.2658 - mean_squared_error: 0.0812 - acc: 0.8873
20000/20000 [==============================] - 0s 25us/step - loss: 0.2659 - mean_squared_error: 0.0812 - acc: 0.8872
Epoch 7/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3680 - mean_squared_error: 0.1021 - acc: 0.9062
 2048/20000 [==>...........................] - ETA: 0s - loss: 0.2624 - mean_squared_error: 0.0802 - acc: 0.8940
 4288/20000 [=====>........................] - ETA: 0s - loss: 0.2619 - mean_squared_error: 0.0804 - acc: 0.8904
 6816/20000 [=========>....................] - ETA: 0s - loss: 0.2673 - mean_squared_error: 0.0818 - acc: 0.8889
 9344/20000 [=============>................] - ETA: 0s - loss: 0.2649 - mean_squared_error: 0.0807 - acc: 0.8900
11872/20000 [================>.............] - ETA: 0s - loss: 0.2631 - mean_squared_error: 0.0802 - acc: 0.8902
14368/20000 [====================>.........] - ETA: 0s - loss: 0.2626 - mean_squared_error: 0.0801 - acc: 0.8900
16384/20000 [=======================>......] - ETA: 0s - loss: 0.2641 - mean_squared_error: 0.0808 - acc: 0.8885
18624/20000 [==========================>...] - ETA: 0s - loss: 0.2649 - mean_squared_error: 0.0811 - acc: 0.8880
20000/20000 [==============================] - 0s 24us/step - loss: 0.2644 - mean_squared_error: 0.0809 - acc: 0.8884
Epoch 8/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2175 - mean_squared_error: 0.0720 - acc: 0.8750
 2208/20000 [==>...........................] - ETA: 0s - loss: 0.2820 - mean_squared_error: 0.0860 - acc: 0.8818
 4640/20000 [=====>........................] - ETA: 0s - loss: 0.2802 - mean_squared_error: 0.0851 - acc: 0.8847
 7104/20000 [=========>....................] - ETA: 0s - loss: 0.2732 - mean_squared_error: 0.0832 - acc: 0.8872
 9632/20000 [=============>................] - ETA: 0s - loss: 0.2649 - mean_squared_error: 0.0808 - acc: 0.8901
11648/20000 [================>.............] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.0808 - acc: 0.8896
13952/20000 [===================>..........] - ETA: 0s - loss: 0.2634 - mean_squared_error: 0.0808 - acc: 0.8899
16000/20000 [=======================>......] - ETA: 0s - loss: 0.2639 - mean_squared_error: 0.0810 - acc: 0.8889
18016/20000 [==========================>...] - ETA: 0s - loss: 0.2632 - mean_squared_error: 0.0806 - acc: 0.8895
20000/20000 [==============================] - 1s 25us/step - loss: 0.2624 - mean_squared_error: 0.0804 - acc: 0.8898
Epoch 9/10

   32/20000 [..............................] - ETA: 0s - loss: 0.4098 - mean_squared_error: 0.1351 - acc: 0.8125
 2400/20000 [==>...........................] - ETA: 0s - loss: 0.2689 - mean_squared_error: 0.0823 - acc: 0.8842
 4928/20000 [======>.......................] - ETA: 0s - loss: 0.2756 - mean_squared_error: 0.0845 - acc: 0.8847
 7424/20000 [==========>...................] - ETA: 0s - loss: 0.2742 - mean_squared_error: 0.0839 - acc: 0.8842
 9568/20000 [=============>................] - ETA: 0s - loss: 0.2707 - mean_squared_error: 0.0831 - acc: 0.8855
12096/20000 [=================>............] - ETA: 0s - loss: 0.2672 - mean_squared_error: 0.0818 - acc: 0.8875
14272/20000 [====================>.........] - ETA: 0s - loss: 0.2646 - mean_squared_error: 0.0808 - acc: 0.8889
16768/20000 [========================>.....] - ETA: 0s - loss: 0.2624 - mean_squared_error: 0.0804 - acc: 0.8883
19296/20000 [===========================>..] - ETA: 0s - loss: 0.2612 - mean_squared_error: 0.0800 - acc: 0.8886
20000/20000 [==============================] - 0s 25us/step - loss: 0.2620 - mean_squared_error: 0.0803 - acc: 0.8879
Epoch 10/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3509 - mean_squared_error: 0.1060 - acc: 0.8438
 2400/20000 [==>...........................] - ETA: 0s - loss: 0.2710 - mean_squared_error: 0.0812 - acc: 0.8862
 4352/20000 [=====>........................] - ETA: 0s - loss: 0.2660 - mean_squared_error: 0.0800 - acc: 0.8874
 6368/20000 [========>.....................] - ETA: 0s - loss: 0.2668 - mean_squared_error: 0.0810 - acc: 0.8869
 8416/20000 [===========>..................] - ETA: 0s - loss: 0.2657 - mean_squared_error: 0.0802 - acc: 0.8884
10944/20000 [===============>..............] - ETA: 0s - loss: 0.2633 - mean_squared_error: 0.0802 - acc: 0.8887
13472/20000 [===================>..........] - ETA: 0s - loss: 0.2627 - mean_squared_error: 0.0799 - acc: 0.8890
15968/20000 [======================>.......] - ETA: 0s - loss: 0.2621 - mean_squared_error: 0.0798 - acc: 0.8893
18016/20000 [==========================>...] - ETA: 0s - loss: 0.2626 - mean_squared_error: 0.0801 - acc: 0.8884
20000/20000 [==============================] - 1s 25us/step - loss: 0.2628 - mean_squared_error: 0.0803 - acc: 0.8882
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 1 0 1 1 1 0 1 1] [0 0 1 0 0 0 1 0 1 1]
... MSE: 0.352108 PEARSON: (0.33084343199025024, 0.0) F-SCORE: 0.505141
+++++++++++++++++++++++++++++++++ Max Entropy ++++++++++++++++++++++++++++++++++
Train and predict ...
training max_ent with penalty=l2, solver=sag, C=1.0, and class_weight=balanced
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 0 1 1 1] [0 0 1 0 0 0 0 0 0 0]
mean squared error: 0.2977746991081479
pearson coefficient: (0.34542166858359435, 0.0)
f-score: 0.5167228780213603
... MSE: 0.297775 PEARSON: (0.34542166858359435, 0.0) F-SCORE: 0.516723
++++++++++++++++++++++++++++ Support Vector Machine ++++++++++++++++++++++++++++
Train and predict ...
training svm with kernel='rbf', C=1.0, gamma='scale', class_weight='balanced'
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 1 0 1 1] [0 0 1 0 0 0 0 0 0 1]
mean squared error: 0.29781799289981814
pearson coefficient: (0.35456401958938477, 0.0)
f-score: 0.5227887617065556
... MSE: 0.297818 PEARSON: (0.35456401958938477, 0.0) F-SCORE: 0.522789
====December 10, 2018 05:46:00 PM====
