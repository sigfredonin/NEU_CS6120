====December 10, 2018 07:06:13 PM====
------------------------------- Hyper-parameters -------------------------------
USE_FULL_TRAIN: False, TRAIN_SIZE: 20000
Full training set size - sarcastic: 20000, non-sarcastic: 100000
Senti-scores - # most common n-grams: 25000, remove common n-grams: True
Senti-scores - min senses: 3, max senses: 12
Senti-scores - remove stopwords: False remove punctuation: False
Senti-scores - punctuation: .. ... ; . , :
--------------------------------------------------------------------------------
num sarcastic tweets: 25273
num non sarcastic tweets: 117825
Scoring sentiment in 20000 tweets ...
..................................................
..................................................
..................................................
..................................................
[(0, 2892), (2, 1418), (1, 1205), (3, 1015), (-2, 959), (4, 903), (-1, 888), (5, 829), (-3, 819), (-4, 621), (6, 557), (7, 490), (-5, 490), (8, 424), (11, 386), (9, 378), (10, 339), (-6, 337), (13, 309), (12, 274)]
Tweets with scored words: 18525; total words scored: 63595
Tweets with exceptions: 19729; total exceptions: 1138583
Total word/tags with scores: 5060
Scoring sentiment in 23098 tweets ...
..................................................
..................................................
..................................................
..................................................
..............................[(0, 3412), (2, 1735), (1, 1443), (-2, 1284), (3, 1184), (-1, 1128), (4, 1064), (-3, 990), (5, 905), (-4, 753), (6, 680), (-5, 590), (7, 558), (8, 452), (-6, 445), (11, 395), (9, 368), (10, 352), (-7, 335), (-8, 307)]
Tweets with scored words: 21341; total words scored: 73110
Tweets with exceptions: 22800; total exceptions: 1291988
Total word/tags with scores: 6833
Training features:
[[ 6  6  6  0  0  4 -8]
 [13 15 13  2  0  7 -3]]
Training labels:
[1 1 1 1 1 1 1 1 1 1]
Testing features:
[[7 1 7 2 0 0 1]
 [4 2 4 0 0 5 0]]
Testing labels:
[1 1 1 1 1 1 1 1 1 1]
====December 10, 2018 07:09:54 PM====
++++++++++++++++++++++++++++ Multilayer Perceptron +++++++++++++++++++++++++++++
Train and predict ...
====December 10, 2018 07:09:56 PM====
--- Model ---
Input    : 1 x 7
Layer h1 : 7 x 20 relu
Layer h2 : 20 x 10 relu
Output   : 10 x 1 SIGMOID
----------
====December 10, 2018 07:09:56 PM====
Epoch 1/10

   32/20000 [..............................] - ETA: 3:04 - loss: 1.3783 - mean_squared_error: 0.2497 - acc: 0.6875
 2176/20000 [==>...........................] - ETA: 2s - loss: 1.1570 - mean_squared_error: 0.3074 - acc: 0.5754  
 4640/20000 [=====>........................] - ETA: 1s - loss: 0.7909 - mean_squared_error: 0.2230 - acc: 0.6918
 7136/20000 [=========>....................] - ETA: 0s - loss: 0.6258 - mean_squared_error: 0.1791 - acc: 0.7552
 9152/20000 [============>.................] - ETA: 0s - loss: 0.5556 - mean_squared_error: 0.1604 - acc: 0.7807
11296/20000 [===============>..............] - ETA: 0s - loss: 0.5056 - mean_squared_error: 0.1467 - acc: 0.7996
13792/20000 [===================>..........] - ETA: 0s - loss: 0.4659 - mean_squared_error: 0.1359 - acc: 0.8141
16288/20000 [=======================>......] - ETA: 0s - loss: 0.4390 - mean_squared_error: 0.1286 - acc: 0.8242
18816/20000 [===========================>..] - ETA: 0s - loss: 0.4196 - mean_squared_error: 0.1234 - acc: 0.8314
20000/20000 [==============================] - 1s 39us/step - loss: 0.4111 - mean_squared_error: 0.1210 - acc: 0.8347
Epoch 2/10

   32/20000 [..............................] - ETA: 9s - loss: 0.2513 - mean_squared_error: 0.0833 - acc: 0.8438
 2592/20000 [==>...........................] - ETA: 0s - loss: 0.2637 - mean_squared_error: 0.0801 - acc: 0.8939
 4576/20000 [=====>........................] - ETA: 0s - loss: 0.2613 - mean_squared_error: 0.0799 - acc: 0.8896
 6688/20000 [=========>....................] - ETA: 0s - loss: 0.2707 - mean_squared_error: 0.0822 - acc: 0.8858
 8704/20000 [============>.................] - ETA: 0s - loss: 0.2717 - mean_squared_error: 0.0829 - acc: 0.8869
10912/20000 [===============>..............] - ETA: 0s - loss: 0.2752 - mean_squared_error: 0.0840 - acc: 0.8857
13472/20000 [===================>..........] - ETA: 0s - loss: 0.2768 - mean_squared_error: 0.0841 - acc: 0.8856
16000/20000 [=======================>......] - ETA: 0s - loss: 0.2759 - mean_squared_error: 0.0840 - acc: 0.8848
18528/20000 [==========================>...] - ETA: 0s - loss: 0.2770 - mean_squared_error: 0.0841 - acc: 0.8851
20000/20000 [==============================] - 1s 26us/step - loss: 0.2766 - mean_squared_error: 0.0841 - acc: 0.8848
Epoch 3/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3814 - mean_squared_error: 0.1254 - acc: 0.8438
 2016/20000 [==>...........................] - ETA: 0s - loss: 0.2749 - mean_squared_error: 0.0856 - acc: 0.8805
 4672/20000 [======>.......................] - ETA: 0s - loss: 0.2728 - mean_squared_error: 0.0840 - acc: 0.8859
 6976/20000 [=========>....................] - ETA: 0s - loss: 0.2776 - mean_squared_error: 0.0848 - acc: 0.8852
 9472/20000 [=============>................] - ETA: 0s - loss: 0.2723 - mean_squared_error: 0.0832 - acc: 0.8875
11968/20000 [================>.............] - ETA: 0s - loss: 0.2697 - mean_squared_error: 0.0822 - acc: 0.8900
14464/20000 [====================>.........] - ETA: 0s - loss: 0.2672 - mean_squared_error: 0.0813 - acc: 0.8904
16512/20000 [=======================>......] - ETA: 0s - loss: 0.2675 - mean_squared_error: 0.0817 - acc: 0.8894
18816/20000 [===========================>..] - ETA: 0s - loss: 0.2703 - mean_squared_error: 0.0824 - acc: 0.8886
20000/20000 [==============================] - 0s 24us/step - loss: 0.2703 - mean_squared_error: 0.0822 - acc: 0.8886
Epoch 4/10

   32/20000 [..............................] - ETA: 9s - loss: 0.1277 - mean_squared_error: 0.0313 - acc: 0.9688
 2560/20000 [==>...........................] - ETA: 0s - loss: 0.2540 - mean_squared_error: 0.0776 - acc: 0.8941
 5088/20000 [======>.......................] - ETA: 0s - loss: 0.2675 - mean_squared_error: 0.0826 - acc: 0.8858
 7616/20000 [==========>...................] - ETA: 0s - loss: 0.2653 - mean_squared_error: 0.0814 - acc: 0.8873
10112/20000 [==============>...............] - ETA: 0s - loss: 0.2684 - mean_squared_error: 0.0819 - acc: 0.8876
12640/20000 [=================>............] - ETA: 0s - loss: 0.2698 - mean_squared_error: 0.0822 - acc: 0.8880
14880/20000 [=====================>........] - ETA: 0s - loss: 0.2708 - mean_squared_error: 0.0830 - acc: 0.8863
17408/20000 [=========================>....] - ETA: 0s - loss: 0.2696 - mean_squared_error: 0.0822 - acc: 0.8884
19488/20000 [============================>.] - ETA: 0s - loss: 0.2696 - mean_squared_error: 0.0823 - acc: 0.8879
20000/20000 [==============================] - 1s 26us/step - loss: 0.2697 - mean_squared_error: 0.0823 - acc: 0.8881
Epoch 5/10

   32/20000 [..............................] - ETA: 1s - loss: 0.3217 - mean_squared_error: 0.1030 - acc: 0.8438
 2400/20000 [==>...........................] - ETA: 0s - loss: 0.2689 - mean_squared_error: 0.0819 - acc: 0.8888
 4608/20000 [=====>........................] - ETA: 0s - loss: 0.2597 - mean_squared_error: 0.0794 - acc: 0.8902
 7136/20000 [=========>....................] - ETA: 0s - loss: 0.2627 - mean_squared_error: 0.0801 - acc: 0.8892
 9152/20000 [============>.................] - ETA: 0s - loss: 0.2644 - mean_squared_error: 0.0804 - acc: 0.8876
11552/20000 [================>.............] - ETA: 0s - loss: 0.2662 - mean_squared_error: 0.0809 - acc: 0.8876
14048/20000 [====================>.........] - ETA: 0s - loss: 0.2668 - mean_squared_error: 0.0809 - acc: 0.8886
16576/20000 [=======================>......] - ETA: 0s - loss: 0.2662 - mean_squared_error: 0.0808 - acc: 0.8890
19104/20000 [===========================>..] - ETA: 0s - loss: 0.2670 - mean_squared_error: 0.0813 - acc: 0.8883
20000/20000 [==============================] - 0s 25us/step - loss: 0.2667 - mean_squared_error: 0.0812 - acc: 0.8886
Epoch 6/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3149 - mean_squared_error: 0.0959 - acc: 0.9062
 2240/20000 [==>...........................] - ETA: 0s - loss: 0.2775 - mean_squared_error: 0.0835 - acc: 0.8866
 4288/20000 [=====>........................] - ETA: 0s - loss: 0.2784 - mean_squared_error: 0.0847 - acc: 0.8846
 6272/20000 [========>.....................] - ETA: 0s - loss: 0.2676 - mean_squared_error: 0.0816 - acc: 0.8892
 8256/20000 [===========>..................] - ETA: 0s - loss: 0.2666 - mean_squared_error: 0.0815 - acc: 0.8884
10496/20000 [==============>...............] - ETA: 0s - loss: 0.2699 - mean_squared_error: 0.0823 - acc: 0.8878
12992/20000 [==================>...........] - ETA: 0s - loss: 0.2669 - mean_squared_error: 0.0816 - acc: 0.8886
15488/20000 [======================>.......] - ETA: 0s - loss: 0.2680 - mean_squared_error: 0.0819 - acc: 0.8883
17856/20000 [=========================>....] - ETA: 0s - loss: 0.2664 - mean_squared_error: 0.0814 - acc: 0.8892
19840/20000 [============================>.] - ETA: 0s - loss: 0.2670 - mean_squared_error: 0.0815 - acc: 0.8889
20000/20000 [==============================] - 1s 26us/step - loss: 0.2669 - mean_squared_error: 0.0814 - acc: 0.8889
Epoch 7/10

   32/20000 [..............................] - ETA: 0s - loss: 0.1744 - mean_squared_error: 0.0518 - acc: 0.9375
 2048/20000 [==>...........................] - ETA: 0s - loss: 0.2642 - mean_squared_error: 0.0805 - acc: 0.8862
 4608/20000 [=====>........................] - ETA: 0s - loss: 0.2673 - mean_squared_error: 0.0815 - acc: 0.8874
 7008/20000 [=========>....................] - ETA: 0s - loss: 0.2691 - mean_squared_error: 0.0821 - acc: 0.8878
 9504/20000 [=============>................] - ETA: 0s - loss: 0.2625 - mean_squared_error: 0.0799 - acc: 0.8913
12000/20000 [=================>............] - ETA: 0s - loss: 0.2634 - mean_squared_error: 0.0801 - acc: 0.8908
14496/20000 [====================>.........] - ETA: 0s - loss: 0.2636 - mean_squared_error: 0.0802 - acc: 0.8898
16576/20000 [=======================>......] - ETA: 0s - loss: 0.2647 - mean_squared_error: 0.0807 - acc: 0.8883
18848/20000 [===========================>..] - ETA: 0s - loss: 0.2670 - mean_squared_error: 0.0814 - acc: 0.8878
20000/20000 [==============================] - 0s 24us/step - loss: 0.2670 - mean_squared_error: 0.0814 - acc: 0.8876
Epoch 8/10

   32/20000 [..............................] - ETA: 0s - loss: 0.1969 - mean_squared_error: 0.0595 - acc: 0.9062
 1984/20000 [=>............................] - ETA: 0s - loss: 0.2656 - mean_squared_error: 0.0799 - acc: 0.8957
 4480/20000 [=====>........................] - ETA: 0s - loss: 0.2618 - mean_squared_error: 0.0799 - acc: 0.8908
 7072/20000 [=========>....................] - ETA: 0s - loss: 0.2679 - mean_squared_error: 0.0815 - acc: 0.8893
 9536/20000 [=============>................] - ETA: 0s - loss: 0.2699 - mean_squared_error: 0.0823 - acc: 0.8872
12096/20000 [=================>............] - ETA: 0s - loss: 0.2702 - mean_squared_error: 0.0825 - acc: 0.8874
14144/20000 [====================>.........] - ETA: 0s - loss: 0.2679 - mean_squared_error: 0.0817 - acc: 0.8882
16800/20000 [========================>.....] - ETA: 0s - loss: 0.2680 - mean_squared_error: 0.0817 - acc: 0.8881
18944/20000 [===========================>..] - ETA: 0s - loss: 0.2676 - mean_squared_error: 0.0816 - acc: 0.8877
20000/20000 [==============================] - 1s 25us/step - loss: 0.2656 - mean_squared_error: 0.0810 - acc: 0.8886
Epoch 9/10

   32/20000 [..............................] - ETA: 0s - loss: 0.3118 - mean_squared_error: 0.0972 - acc: 0.8750
 2432/20000 [==>...........................] - ETA: 0s - loss: 0.2569 - mean_squared_error: 0.0775 - acc: 0.8964
 4960/20000 [======>.......................] - ETA: 0s - loss: 0.2533 - mean_squared_error: 0.0762 - acc: 0.8970
 7456/20000 [==========>...................] - ETA: 0s - loss: 0.2542 - mean_squared_error: 0.0771 - acc: 0.8938
 9472/20000 [=============>................] - ETA: 0s - loss: 0.2537 - mean_squared_error: 0.0769 - acc: 0.8944
11712/20000 [================>.............] - ETA: 0s - loss: 0.2584 - mean_squared_error: 0.0784 - acc: 0.8932
14240/20000 [====================>.........] - ETA: 0s - loss: 0.2612 - mean_squared_error: 0.0795 - acc: 0.8910
16832/20000 [========================>.....] - ETA: 0s - loss: 0.2634 - mean_squared_error: 0.0804 - acc: 0.8887
19392/20000 [============================>.] - ETA: 0s - loss: 0.2641 - mean_squared_error: 0.0805 - acc: 0.8895
20000/20000 [==============================] - 0s 24us/step - loss: 0.2649 - mean_squared_error: 0.0808 - acc: 0.8891
Epoch 10/10

   32/20000 [..............................] - ETA: 0s - loss: 0.2572 - mean_squared_error: 0.0869 - acc: 0.8750
 2528/20000 [==>...........................] - ETA: 0s - loss: 0.2697 - mean_squared_error: 0.0820 - acc: 0.8869
 4608/20000 [=====>........................] - ETA: 0s - loss: 0.2600 - mean_squared_error: 0.0787 - acc: 0.8932
 6624/20000 [========>.....................] - ETA: 0s - loss: 0.2619 - mean_squared_error: 0.0799 - acc: 0.8909
 8736/20000 [============>.................] - ETA: 0s - loss: 0.2681 - mean_squared_error: 0.0821 - acc: 0.8860
10944/20000 [===============>..............] - ETA: 0s - loss: 0.2645 - mean_squared_error: 0.0811 - acc: 0.8880
13472/20000 [===================>..........] - ETA: 0s - loss: 0.2662 - mean_squared_error: 0.0813 - acc: 0.8875
16000/20000 [=======================>......] - ETA: 0s - loss: 0.2685 - mean_squared_error: 0.0820 - acc: 0.8872
18464/20000 [==========================>...] - ETA: 0s - loss: 0.2667 - mean_squared_error: 0.0816 - acc: 0.8874
20000/20000 [==============================] - 0s 24us/step - loss: 0.2652 - mean_squared_error: 0.0812 - acc: 0.8877
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 1 0 1 1] [0 0 1 0 0 0 0 0 1 1]
... MSE: 0.297428 PEARSON: (0.3477819083881882, 0.0) F-SCORE: 0.518300
+++++++++++++++++++++++++++++++++ Max Entropy ++++++++++++++++++++++++++++++++++
Train and predict ...
training max_ent with penalty=l2, solver=sag, C=1.0, and class_weight=balanced
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 0 1 1 1] [0 0 1 0 0 0 0 0 0 0]
mean squared error: 0.2977746991081479
pearson coefficient: (0.34542166858359435, 0.0)
f-score: 0.5167228780213603
... MSE: 0.297775 PEARSON: (0.34542166858359435, 0.0) F-SCORE: 0.516723
++++++++++++++++++++++++++++ Support Vector Machine ++++++++++++++++++++++++++++
Train and predict ...
training svm with kernel='rbf', C=1.0, gamma='scale', class_weight='balanced'
test [1 1 1 1 1 1 1 1 1 1] [0 0 0 0 0 0 0 0 0 0]
pred [0 1 0 0 0 0 1 0 1 1] [0 0 1 0 0 0 0 0 0 1]
mean squared error: 0.29781799289981814
pearson coefficient: (0.35456401958938477, 0.0)
f-score: 0.5227887617065556
... MSE: 0.297818 PEARSON: (0.35456401958938477, 0.0) F-SCORE: 0.522789
====December 10, 2018 07:10:11 PM====
