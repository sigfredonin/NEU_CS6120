Google News word2vec embeddings
  https://code.google.com/archive/p/word2vec/

Word and phrase embeddings, pre-trained vectors trained on part
of Google News dataset (about 100 billion words). The model contains
300-dimensional vectors for 3 million words and phrases.

File: GoogleNews-vectors-negative300.bin.gz
  3,644,258,522 bytes .bin
  1,647,046,227 bytes .gz

Format
  "3000000 300"     Header, number of words, space, vector dimensions, newline
  "<\s> "           word, space
  FFFFFFFF          300 4-byte floating point numbers
  FFFFFFFF
  ...
  FFFFFFFF
  "in "             word, space
  FFFFFFFF          300 4-byte floating point numbers
  FFFFFFFF
  ...
  FFFFFFFF
  ...               ... for 2,999,998 more words

Note: the words are UTF-8 encoded.  Python treats bytes differently from characters
      in strings: bytes have to be decoded with byteString.decode() to make a string
      that can be compared with vocabulary words and used as a key.  The file has
      words with multi-byte characters in it, so you have to read a whole word as a
      byte string before decoding it.

Ways to deal with this file ...

(1) Read the whole thing into memory (YUK!), but there may be a utility.
(2) Make an index, then use f.seek(offset, 0) to read an embedding vector.
(3) Scan the file once, loading the vectors for the words in the vocabulary.
